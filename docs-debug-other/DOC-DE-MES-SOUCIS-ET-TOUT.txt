- La detection cetait good le filter et tt mais j;ai fait en sorte de filtre que 
les lines qui etaient suffisamment proches les unes des autres en disant que si une 
line etait pas entre genre 50 et 200 a cote dune autre ba tu pouvait la jerter
- Then it was very hard to see the circles because HoughCircle was looking for perfect shit and stuff, then I found this thread:
https://stackoverflow.com/questions/9860667/writing-robust-color-and-size-invariant-circle-detection-with-opencv-based-on  
=> And tried MSER  (blob reco approach)  AND IT WORKED HELLLLA GOOD AND Actually detected both cross and circle centers
- Then I needed to separate Cross from circle.






- SO AS I WAS SAYING, The SIMULATION WAS WACKKKK WORKING because of my vision pipeline, SPECIFICALLY the shape detection, i.e template matching is dogshit as i saw on reddit,
  so even though i had not tried everything with it, tweak parameters for hours and so forth, this reddit comment got me thinking : "Useless to try to make this work if my goal is 
  real conditions you know.."
  => THUS i finished the pipeline, perception->minmax->move_orderer and moved on to the real robot for:
    * work directly on real conditions for perception
    * position/shape settings for real robot
    * Work in parallel on the web-Interface during wait-time for real-robot testing.


- I WOULD HAVE DONE THE SCRUCTURE FIRST LIKE THE NODE STRUCT FIRST THEN MOVE ON TO CODING YOU KNOW, BECAUSE I WOULD HAVE AVOIDED DATA PASSING FROM ONE PLACE BACK TO ANOTHER AND UNCLEANLINESS




- REAL ROBOT PERCEPTION
    1. First I chose to try cropping the image because:
        - My MSER was detecting false positive and it was hard to filter everything out only by blob size interval
        - Same goes for the HoughLines algo, which had a lot of lines, difficult to filter all out in sim THUS even harder in real conditions
       * findContours with a size interval for the tablet.
       * Why no Canny or Sobel ? they are not good in bad lightning conditions apparently
       * Then change to an adaptative threshold
    2. Then Warping the image
       * Affine transform ? Nope, because it can only make parallelograms, and my camera definitely has a trapezoid
         => Perspective transform.
    3. Then another binary threshold on the shadow removed image with an adaptive threshold thats generally lower to catch the grid and shapes this time
        * Why no Canny ? Tested, works as good as binary, but multiple circles inside each other as it checks both inside and outside edges and its unnefficient
        * Why adaptive ? Because regular threshold caused some shapes to be only partially perceived
        * YEP with gaussian and tuning it was goood
    4. Grid lines with HoughLines
        * One HoughLines for vertical, one for horizontal. Why ? Because the votes parameter needed for horizontal lines to filter noisy horizontal lines is too high,
          and filters out all vertical lines.
          => Thus 2 HoughLines: HoughLines(horiz).score HIGH -- HoughLines(vertical).score LOW
        * Then manual clustering + calculation of the middle lines
        * In the future I would use kmeans maybbe : https://stackoverflow.com/questions/46565975/find-intersection-point-of-two-lines-drawn-using-houghlines-opencv => 
          "With HoughLines, you already have the result as rho, theta so you can easily segment into two classes of angle with theta. You can use for e.g. cv2.kmeans() 
          with theta as your data you want to split."
        * Then calculate centers and shit
    5. Grid Shapes using MSER/SIFT
        * SIFT for Each square ? NOPE => MSER on whole cropped image => Identify which squares are populated => SIFT on populated squares ONLY.
        * MSER Wak WACK (see screenshot), so NOPE => exploring other things
        * LightWeight CNN ? SIFT ? Contour Analysis ?
        * Filling the shapes was wack
        * SIFT was ?? lot of keypoints but what to do ?
        * Contours analysis was GREAT, nice contours on everything, no noise
        * Contour detection per square Doesnt work right off the bat, I need to make the shape lines thinner in order to avoid contouring the whole shape instead of each individually.
        * TRY ADAPTATIVE CANNY EDGES => OH OUI C UN BANGER
        * Ensuite contour detect
        * Ensuite area >50 pour filter out les cases vides
        * Puis circularity pour differencier cross et circle => ALMOST ALWAYS: cross<0,13 &&   0,8<circle<
        * Actually Circularity was great BUT not for circles that were a little bit cropped or unfinished (not meeting the other end of the line of the circle you know, leaving a kind of hollow shape)
          and those weird circles were sooo hard to identify properly as circles 
          => Switch to solidity + vertices id (solidity = area/convex_hull)   you know => Circles have HIGH number of    solidity ~0.9
             And Cross have low solidity (easy to understand).
    6. SUCCESS RATE analysis:
        - Standard Gray-Image: 35% Success
        - With HSV  channel 2 (V):   45% Success

    RESSOURCES:
    https://stackoverflow.com/questions/42203898/python-opencv-blob-detection-or-circle-detection
    https://medium.com/aimonks/decoding-the-power-of-sift-the-gold-standard-in-feature-detection-7025e6eef7ab
    https://medium.com/@noel.benji/a-guide-to-robust-edge-detection-with-opencv-1d703506e014
    https://medium.com/@amit25173/opencv-object-recognition-642c8cf8379b
    file:///home/emskiller/Downloads/Learning%20OpenCV%20OReilly.pdf


- In order to be faster in reco, you could keep the current state of the game in the deque buffers => yep


- The urge to finish a functional project pushed me to just dive down on code with a retro-planning that was I think functional, 
  but not enough code structure analysis (like what nodes do I create, what architecture Do i want for the system ?) 
  => Result: some of the code is in my opinion not clean/readable enough for me for a final project. (Data reused, variables added as return in some function) -> EXAMPLES PLEASE

- anticipate more amount of time needed for runtime debugging (i.e Finding all Segmentation fault/opencv fault Crashes   sources)


- POUR LA DETECTION DES LIGNES CETAIT CA SON WORKFLOW:
Compact summary:
1. ROI mask - only detect lines in center 70% of image
2. Intersection-based - find where horizontal/vertical lines cross (not just individual lines)
3. Cluster intersections - merge nearby intersection points
4. Extract line positions - get unique h/v coordinates from intersections
5. Score pairs - prefer line pairs with reasonable spacing (80-300px) that are near image center
6. Pick best - return highest-scoring pair instead of just first 2
Key change: From "detect lines → cluster" to "detect lines → find intersections → cluster intersections → score geometry". More robust to noise.